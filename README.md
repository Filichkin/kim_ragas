# RAGAS Data Generator

Генератор данных для оценки RAG (Retrieval-Augmented Generation) систем с использованием библиотеки RAGAS.

## Возможности

- **Автоматическое разбиение документов на чанки** с различными стратегиями
- **Формирование связей между документами и чанками** на основе эмбедингов и перекрытия сущностей
- **Генерация вопросов различных типов и стилей**
- **Создание сценариев с заданным распределением**
- **Гибкая конфигурация** через предустановки или пользовательские настройки

## Установка

1. Установите зависимости:
```bash
pip install -r requirements.txt
```

2. Установите модели spaCy (опционально, для извлечения сущностей):
```bash
python -m spacy download ru_core_news_sm
python -m spacy download en_core_web_sm
```

## Быстрый старт

### Базовое использование

```python
from ragas_data_generator import RAGASDataGenerator, ChunkingStrategy

# Создание генератора
generator = RAGASDataGenerator(
    chunking_strategy=ChunkingStrategy.RECURSIVE,
    embedding_model='all-MiniLM-L6-v2'
)

# Генерация датасета
generator.generate_dataset(
    document_paths=['./data/document.pdf'],
    output_path='./output/ragas_dataset.json',
    num_scenarios=50
)
```

### Использование предустановленных конфигураций

```python
from ragas_config import get_config

# Быстрая генерация
config = get_config('fast')
# ... применение конфигурации

# Комплексная генерация
config = get_config('comprehensive')
# ... применение конфигурации
```

## Стратегии разбиения на чанки

### 1. Рекурсивное разбиение (RECURSIVE)
- Разбивает текст по иерархии разделителей
- Подходит для большинства типов документов
- Настраиваемый размер чанка и перекрытие

### 2. Разбиение по токенам (TOKEN_BASED)
- Разбивает текст на основе токенов
- Точный контроль размера чанка
- Подходит для моделей с ограничениями по токенам

### 3. NLTK-разбиение (NLTK_BASED)
- Использует NLTK для разбиения на предложения
- Сохраняет семантическую целостность
- Хорошо работает с естественным языком

### 4. Семантическое разбиение (SEMANTIC)
- Группирует предложения по семантической близости
- Использует TF-IDF для вычисления схожести
- Создает более когерентные чанки

## Типы вопросов

### Абстрактные вопросы
- Вопросы о общих принципах и концепциях
- Требуют понимания и обобщения информации
- Пример: "Каковы основные принципы, изложенные в документе?"

### Конкретные вопросы
- Вопросы о точных данных и фактах
- Требуют поиска конкретной информации
- Пример: "Какие конкретные данные представлены?"

### Single-hop вопросы
- Вопросы, требующие информацию из одного источника
- Прямой поиск в одном чанке
- Пример: "Что говорится о [ENTITY] в документе?"

### Multi-hop вопросы
- Вопросы, требующие информацию из нескольких источников
- Требуют связывания информации из разных чанков
- Пример: "Как [ENTITY1] связано с [ENTITY2]?"

## Стили вопросов

- **Формальный**: академический стиль
- **Неформальный**: повседневный стиль
- **Технический**: специализированная терминология
- **Разговорный**: диалоговый стиль

## Типы сценариев

### Простые сценарии
- 5 вопросов
- Преимущественно конкретные и single-hop вопросы
- Низкая сложность

### Средние сценарии
- 10 вопросов
- Сбалансированное распределение типов вопросов
- Средняя сложность

### Сложные сценарии
- 15 вопросов
- Включают multi-hop и абстрактные вопросы
- Высокая сложность

## Конфигурация

### Предустановленные конфигурации

- **default**: стандартные настройки
- **fast**: быстрая генерация с меньшим количеством сценариев
- **comprehensive**: полная генерация с максимальным покрытием
- **technical**: оптимизировано для технических документов

### Пользовательская конфигурация

```python
from ragas_config import create_custom_config, ChunkingConfig

config = create_custom_config(
    chunking=ChunkingConfig(
        strategy=ChunkingStrategy.SEMANTIC,
        chunk_size=800,
        chunk_overlap=100
    ),
    scenario=ScenarioConfig(
        num_scenarios=100,
        scenario_type_distribution={
            'simple': 0.2,
            'medium': 0.5,
            'complex': 0.3
        }
    )
)
```

## Структура выходного файла

```json
{
  "chunks": [
    {
      "chunk_id": "doc_0",
      "document_id": "document.pdf",
      "content": "Содержимое чанка...",
      "metadata": {
        "chunk_index": 0,
        "word_count": 150,
        "char_count": 800,
        "entities": ["сущность1", "сущность2"]
      }
    }
  ],
  "relationships": {
    "embedding_similarity": {
      "chunk_1": [["chunk_2", 0.85], ["chunk_3", 0.72]]
    },
    "entity_overlap": {
      "chunk_1": [["chunk_2", 0.3]]
    }
  },
  "scenarios": [
    {
      "scenario_id": "simple_0",
      "scenario_type": "simple",
      "questions": [
        {
          "question": "Вопрос...",
          "question_type": "concrete",
          "question_style": "formal",
          "expected_answer": "Ожидаемый ответ...",
          "relevant_chunks": ["chunk_1", "chunk_2"],
          "context_chunks": ["chunk_1"],
          "difficulty_score": 0.6
        }
      ],
      "metadata": {
        "total_questions": 5,
        "avg_difficulty": 0.6,
        "chunk_coverage": 3
      }
    }
  ],
  "metadata": {
    "total_chunks": 25,
    "total_scenarios": 50,
    "total_questions": 500
  }
}
```

## Примеры использования

См. файл `example_usage.py` для подробных примеров:

- Базовое использование
- Пользовательские конфигурации
- Различные стратегии разбиения
- Анализ сгенерированных вопросов
- Пакетная обработка документов

## Требования

- Python 3.8+
- См. `requirements.txt` для полного списка зависимостей

## Поддерживаемые форматы

- PDF документы (через PyPDFLoader)

## Ограничения

- Извлечение сущностей требует установки моделей spaCy
- Семантическое разбиение работает лучше с документами на английском языке
- Большие документы могут требовать значительных вычислительных ресурсов

## Лицензия

MIT License
